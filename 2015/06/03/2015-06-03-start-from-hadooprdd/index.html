<!DOCTYPE html><html lang="null"><head><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description"><title>Start from HadoopRDD | ShallWe Talk</title><link rel="stylesheet" type="text/css" href="/css/style.css?v=0.0.0"><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/normalize/7.0.0/normalize.min.css"><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/pure/1.0.0/pure-min.css"><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/pure/1.0.0/grids-responsive-min.css"><link rel="stylesheet" href="//cdn.bootcss.com/font-awesome/4.7.0/css/font-awesome.min.css"><script type="text/javascript" src="//cdn.bootcss.com/jquery/3.2.1/jquery.min.js"></script><link rel="Shortcut Icon" type="image/x-icon" href="/favicon.ico"><link rel="apple-touch-icon" href="/apple-touch-icon.png"><link rel="apple-touch-icon-precomposed" href="/apple-touch-icon.png"></head><body><div class="body_container"><div id="header"><div class="site-name"><h1 class="hidden">Start from HadoopRDD</h1><a id="logo" href="/.">ShallWe Talk</a><p class="description"></p></div><div id="nav-menu"><a href="/." class="current"><i class="fa fa-home"> Start</i></a><a href="/archives/"><i class="fa fa-archive"> Archiv</i></a><a href="/about/"><i class="fa fa-user"> Über</i></a><a href="/atom.xml"><i class="fa fa-rss"> RSS</i></a></div></div><div id="layout" class="pure-g"><div class="pure-u-1 pure-u-md-3-4"><div class="content_container"><div class="post"><h1 class="post-title">Start from HadoopRDD</h1><div class="post-meta">Jun 3, 2015<span> | </span><span class="category"><a href="/categories/source-code/">source code</a><a href="/categories/source-code/Spark/">Spark</a></span></div><div class="post-content"><p>我们在<a href="http://6carol6.github.io/blog/2015/04/17/rdd-in-spark/" target="_blank" rel="external">之前的文章</a>中介绍过一些RDD的创建，然而并没有解释过各个RDD背后的细节，各个RDD是怎么被拆分到不同的Partition的？拆成几份？每份存在哪？这都是我们还没有解决的问题。本文将从HadoopRDD着手，追溯HadoopRDD的生成过程与实现细节，来看一看Spark中的数据分区是怎么回事。 </p>
<p>HadoopRDD在应用程序中调用SparkContext的textFile，从而调用hadoopFile被创建。  </p>
<a id="more"></a>
<p>textFile: Read a text file from HDFS, a local system(available on all nodes), ir any Hadoop-supported file system URI, and return it as an RDD of Strings.<br>hadoopFile: Get an RDD for a Hadoop file with an arbitrary InputFormat 不可以直接cache，需要先map，否则会创建很多副本。  </p>
<p>先以ctx.textFile为例。</p>
<pre><code>val lines = ctx.textFile(arg(0), 1)
</code></pre><p>第一个参数是文件的位置，第二个参数是MinPartitions。  </p>
<p>============================<strong>关于MinPartitions的讨论</strong>====================================  </p>
<p>我们稍微看一下第二个参数。在SparkContext里有对第二个参数的默认值规定：  </p>
<pre><code>def defaultMinPartitions: Int = math.min(defaultParallelism, 2)
</code></pre><p>所以默认是划分成2份的。然后Parallelism也有默认规定。  </p>
<pre><code>def defaultParallelism: Int = taskScheduler.defaultParallelism
</code></pre><p>在TaskSchedulerImpl里有这样的定义：  </p>
<pre><code>override def defaultParallelism() = backend.defaultParallelism()
</code></pre><p>这里的backend是一个<code>SchedulerBackend</code>。在<code>TaskScheduler</code>初始化的时候传入，<code>SchedulerBackend</code>有两种类型，分别是local和cluster的。<code>LocalBackend</code>对应的是local的。cluster的又有<code>SparkDeploySchedulerBackend</code>和<code>CoarseGrainedSchedulerBackend</code>，还有给mesos用的<code>MesosSchedulerBackend</code>什么的，然后还有个不知道干什么用的<code>SimrSchedulerBackend</code>。一般我们开启Spark的Stand-alone模式，使用的都是<code>SparkDeploySchedulerBackend</code>。这里我们主要关注Standalone模式，所以看<code>SparkDeploySchedulerBackend</code>就好。  </p>
<p>于是回到我们对parallelism的疑问，看一下<code>SparkDeploySchedulerBackend.defaultParallelism()</code>。然后发现这个backend继承自<code>CoarseGrainedSchedulerBackend</code>:   </p>
<pre><code>override def defaultParallelism(): Int = {
  conf.getInt(&quot;spark.default.parallelism&quot;, math.max(totalCoreCount.get(), 2))
}
</code></pre><p>意思就是说如果有设置就读设置啦，没有就选核数和2中较大的那个啦。通俗来说就是有几个核心就分几份。  </p>
<p>=======================================<strong>讨论结束</strong>=========================================  </p>
<p>让我们再回到<code>ctx.textFile(arg(0), 1)</code>  </p>
<!--lang:scala-->
<pre><code>/**
 * Read a text file from HDFS, a local file system (available on all nodes), or any
 * Hadoop-supported file system URI, and return it as an RDD of Strings.
 */
def textFile(path: String, minPartitions: Int = defaultMinPartitions): RDD[String] = {
  hadoopFile(path, classOf[TextInputFormat], classOf[LongWritable], classOf[Text],
    minPartitions).map(pair =&gt; pair._2.toString)
}
</code></pre><p>先生成一个HadoopRDD，然后再通过map操作生成一个MappedRDD，后者就不讲了，小贴一段代码：  </p>
<!--lang:scala-->
<pre><code>/**
 * Return a new RDD by applying a function to all elements of this RDD.
 */
def map[U: ClassTag](f: T =&gt; U): RDD[U] = new MappedRDD(this, sc.clean(f))
</code></pre><p>主要还是看刚才那个HadoopRDD。  </p>
<!--lang:scala-->
<pre><code>/** Get an RDD for a Hadoop file with an arbitrary InputFormat
  *
  * &apos;&apos;&apos;Note:&apos;&apos;&apos; Because Hadoop&apos;s RecordReader class re-uses the same Writable object for each
  * record, directly caching the returned RDD will create many references to the same object.
  * If you plan to directly cache Hadoop writable objects, you should first copy them using
  * a `map` function.
  * */
def hadoopFile[K, V](
    path: String,
    inputFormatClass: Class[_ &lt;: InputFormat[K, V]],
    keyClass: Class[K],
    valueClass: Class[V],
    minPartitions: Int = defaultMinPartitions
    ): RDD[(K, V)] = {
  // A Hadoop configuration can be about 10 KB, which is pretty big, so broadcast it.
  val confBroadcast = broadcast(new SerializableWritable(hadoopConfiguration))
  val setInputPathsFunc = (jobConf: JobConf) =&gt; FileInputFormat.setInputPaths(jobConf, path)
  new HadoopRDD(
    this,
    confBroadcast,
    Some(setInputPathsFunc),
    inputFormatClass,
    keyClass,
    valueClass,
    minPartitions)
}
</code></pre><p>首先看注释。这里提到了说在缓存HadoopRDD之前要进行map操作。所以<code>textFile</code>乖乖照做了。然后首先要广播Hadoop configuration。为什么呢？注释里说因为它的大小是10KB左右，相当大了，所以要broadcast。（什么鬼）总之就是先广播了一下。然后这个广播的返回值是要传给HadoopRDD的。（后面看一下有什么鬼用）  </p>
<p>==================================<strong>插播一段广播</strong>==================================  </p>
<!--lang:scala-->
<pre><code>/**
 * Broadcast a read-only variable to the cluster, returning a
 * [[org.apache.spark.broadcast.Broadcast]] object for reading it in distributed functions.
 * The variable will be sent to each cluster only once.
 */
def broadcast[T: ClassTag](value: T): Broadcast[T] = {
  val bc = env.broadcastManager.newBroadcast[T](value, isLocal)
  cleaner.foreach(_.registerBroadcastForCleanup(bc))
  bc
}
</code></pre><p><strong>我们猜一下广播的意思应该是告诉每一个机器应该拿哪一部分数据？不是要从HDFS里取数据么。所以看下广播出去的数据是<code>hadoopConfiguration</code>。</strong>看一下这是什么鬼。  </p>
<!--lang:scala-->
<pre><code>/** A default Hadoop Configuration for the Hadoop code (e.g. file systems) that we reuse. */
val hadoopConfiguration: Configuration = {
  val env = SparkEnv.get
  val hadoopConf = SparkHadoopUtil.get.newConfiguration()
  // Explicitly check for S3 environment variables
  if (System.getenv(&quot;AWS_ACCESS_KEY_ID&quot;) != null &amp;&amp;
      System.getenv(&quot;AWS_SECRET_ACCESS_KEY&quot;) != null) {
    hadoopConf.set(&quot;fs.s3.awsAccessKeyId&quot;, System.getenv(&quot;AWS_ACCESS_KEY_ID&quot;))
    hadoopConf.set(&quot;fs.s3n.awsAccessKeyId&quot;, System.getenv(&quot;AWS_ACCESS_KEY_ID&quot;))
    hadoopConf.set(&quot;fs.s3.awsSecretAccessKey&quot;, System.getenv(&quot;AWS_SECRET_ACCESS_KEY&quot;))
    hadoopConf.set(&quot;fs.s3n.awsSecretAccessKey&quot;, System.getenv(&quot;AWS_SECRET_ACCESS_KEY&quot;))
  }
  // Copy any &quot;spark.hadoop.foo=bar&quot; system properties into conf as &quot;foo=bar&quot;
  conf.getAll.foreach { case (key, value) =&gt;
    if (key.startsWith(&quot;spark.hadoop.&quot;)) {
      hadoopConf.set(key.substring(&quot;spark.hadoop.&quot;.length), value)
    }
  }
  val bufferSize = conf.get(&quot;spark.buffer.size&quot;, &quot;65536&quot;)
  hadoopConf.set(&quot;io.file.buffer.size&quot;, bufferSize)
  hadoopConf
}
</code></pre><p><strong>好像跟我们猜得不太一样，所以我们的疑问再放一放。</strong>  </p>
<p>==================================<strong>广播结束啦</strong>==================================  </p>
<p>然后设置了一下文件位置就直接<code>new HadoopRDD</code>了。既然新建了一个RDD，那么做了什么事情呢？我们知道一个RDD有五个属性。  </p>
<ul>
<li>A list of partitions</li>
<li>A function for computing each split</li>
<li>A list of dependencies on other RDDs</li>
<li>Optionally, a Partitioner for key-value RDDs (e.g. to say that the RDD is hash-partitioned)</li>
<li>Optionally, a list f preferred locations to compute each split on (e.g. block locations for an HDFS file)</li>
</ul>
<p>首先看第一个属性partitions。Spark会为每个RDD准备一个<code>@transient private var partitions_ : Array[Partition] = null</code>当第一次调用<code>rdd.partitions(partitionId)</code>的时候，就会调用子类（例如HadoopRDD）override的getPartitions。  </p>
<p>这里我们来看一下HadoopRDD的getPartitions:  </p>
<!--lang:scala-->
<pre><code>override def getPartitions: Array[Partition] = {
  val jobConf = getJobConf() //这一句先放一放
  // add the credentials here as this can be called before SparkContext initialized
  SparkHadoopUtil.get.addCredentials(jobConf) //这句见上一个注释
  val inputFormat = getInputFormat(jobConf)
  if (inputFormat.isInstanceOf[Configurable]) {
    inputFormat.asInstanceOf[Configurable].setConf(jobConf)
  }
  val inputSplits = inputFormat.getSplits(jobConf, minPartitions)  //看这里看这里
  val array = new Array[Partition](inputSplits.size)
  for (i &lt;- 0 until inputSplits.size) {
    array(i) = new HadoopPartition(id, i, inputSplits(i)) //看这里看这里
  }
  array
}
</code></pre><p>首先要<code>getInputFormat</code>。<code>InputFormat</code>这个东西应该是来自Hadoop。课本上说它描述了MapReduce作业数据的输入形式和格式。我们接下来就看一下在Spark里，这东西是怎么用的。首先就是一些设置，我们先放着不看。然后从<code>inputFormat.getSplits</code>开始看。这个就是把输入拆成一个一个的split，每一个split作为一个task的输入。即task的数量是又InputSplit决定的。然后就新建了inputSplits.size个HadoopPartition（这个类定义在HadoopRDD.scala里）并作为Array返回。所以这货就是Partition最后所指的东西了。  </p>
<pre><code>/**
 * A Spark split class that wraps around a Hadoop InputSplit.
 */
private[spark] class HadoopPartition(rddId: Int, idx: Int, @transient s: InputSplit)
  extends Partition {

  val inputSplit = new SerializableWritable[InputSplit](s)

  override def hashCode(): Int = 41 * (41 + rddId) + idx

  override val index: Int = idx

  /**
   * Get any environment variables that should be added to the users environment when running pipes
   * @return a Map with the environment variables and corresponding values, it could be empty
   */
  def getPipeEnvVars(): Map[String, String] = {
    val envVars: Map[String, String] = if (inputSplit.value.isInstanceOf[FileSplit]) {
      val is: FileSplit = inputSplit.value.asInstanceOf[FileSplit]
      // map_input_file is deprecated in favor of mapreduce_map_input_file but set both
      // since its not removed yet
      Map(&quot;map_input_file&quot; -&gt; is.getPath().toString(),
        &quot;mapreduce_map_input_file&quot; -&gt; is.getPath().toString())
    } else {
      Map()
    }
    envVars
  }
}
</code></pre><p>然后看一下HadoopRDD的compute函数：  </p>
<!--lang:scala-->
<pre><code>override def compute(theSplit: Partition, context: TaskContext): InterruptibleIterator[(K, V)] = {
  val iter = new NextIterator[(K, V)] {

    val split = theSplit.asInstanceOf[HadoopPartition]
    logInfo(&quot;Input split: &quot; + split.inputSplit)
    var reader: RecordReader[K, V] = null
    val jobConf = getJobConf()
    val inputFormat = getInputFormat(jobConf)
    HadoopRDD.addLocalConfiguration(new SimpleDateFormat(&quot;yyyyMMddHHmm&quot;).format(createTime),
      context.stageId, theSplit.index, context.attemptId.toInt, jobConf)
    reader = inputFormat.getRecordReader(split.inputSplit.value, jobConf, Reporter.NULL)

    // Register an on-task-completion callback to close the input stream.
    context.addOnCompleteCallback{ () =&gt; closeIfNeeded() }
    val key: K = reader.createKey()
    val value: V = reader.createValue()
    override def getNext() = {
      try {
        finished = !reader.next(key, value)
      } catch {
        case eof: EOFException =&gt;
          finished = true
      }
      (key, value)
    }

    override def close() {
      try {
        reader.close()
      } catch {
        case e: Exception =&gt; logWarning(&quot;Exception in RecordReader.close()&quot;, e)
      }
    }
  }
  new InterruptibleIterator[(K, V)](context, iter)
}
</code></pre><p>这个函数由两部分组成，一个是返回值<code>new InterruptibleIterator[(K, V)](context, iter)</code>，另一个是生成这个返回值iter参数的部分。我们先看一下这个iter是怎么回事。  </p>
<p>=============================================<strong>关于iter</strong>====================================  </p>
<p>iter是一个NextIterator类型，这个类型在spark.util.NextIterator中定义。这里进行了一些初始化和<code>getNext()</code>/<code>close()</code>函数的override。  </p>
<p>首先把theSplit用asInstanceOf转换成HadoopPartition（要求theSplit的类型是HadoopPartition的子类）。去追一下这个split可以知道这是一个Partition，来自Task被创建时候传入的partitionId，然后使用rdd.partitions(partitionId)取出。还记得我们当初创建Task的时候从p = 0-&gt;stage.numPartitions创建的吗？然后我们把p传入了Task的参数。就是那个！这时候我们就要把partition根据partitionId也就是p取出了。  </p>
<p>然后就可以通过这个iter.getNext()取出这个split中的(key, value)直到取完。  </p>
<p>小结一下也就是说，有了iter，集群中的每个机器（准确说是每个内核）只要用iter.getNext()就可以取出HDFS中的<key, value="">了。  </key,></p>
<p>===========================================<strong>iter讨论结束</strong>===================================  </p>
<p>所以HadoopRDD的<code>compute()</code>就是提供了一个对HDFS中数据的迭代器。  </p>
</div><div class="tags"></div><div class="post-nav"><a href="/2015/06/14/2015-06-14-leetcode226-invert-binary-tree/" class="pre">LeetCode226: Invert Binary Tree</a><a href="/2015/06/01/2015-06-01-spark-execute/" class="next">Spark Execute</a></div></div></div></div><div class="pure-u-1-4 hidden_mid_and_down"><div id="sidebar"><div class="widget"><form action="//www.google.com/search" method="get" accept-charset="utf-8" target="_blank" class="search-form"><input type="text" name="q" maxlength="20" placeholder="Search"/><input type="hidden" name="sitesearch" value="http://yoursite.com"/></form></div><div class="widget"><div class="widget-title"><i class="fa fa-folder-o"> Kategorien</i></div><ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/Beijing/">Beijing</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/C语言学习笔记/">C语言学习笔记</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Security/">Security</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Spark/">Spark</a><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/categories/Spark/RDD/">RDD</a><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/categories/Spark/RDD/source-code/">source code</a></li></ul></li><li class="category-list-item"><a class="category-list-link" href="/categories/Spark/configuration/">configuration</a></li></ul></li><li class="category-list-item"><a class="category-list-link" href="/categories/blog/">blog</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/coding/">coding</a><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/categories/coding/compiler/">compiler</a></li></ul></li><li class="category-list-item"><a class="category-list-link" href="/categories/dp/">dp</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/language/">language</a><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/categories/language/Japanese/">Japanese</a><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/categories/language/Japanese/lyric/">lyric</a></li></ul></li></ul></li><li class="category-list-item"><a class="category-list-link" href="/categories/leetcode/">leetcode</a><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/categories/leetcode/HashTable/">HashTable</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/leetcode/Two-Pointers/">Two Pointers</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/leetcode/divide-and-conquer/">divide and conquer</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/leetcode/dp/">dp</a></li></ul></li><li class="category-list-item"><a class="category-list-link" href="/categories/mapreduce/">mapreduce</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/music/">music</a><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/categories/music/guitar/">guitar</a></li></ul></li><li class="category-list-item"><a class="category-list-link" href="/categories/paper/">paper</a><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/categories/paper/NLP/">NLP</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/paper/scheduler/">scheduler</a></li></ul></li><li class="category-list-item"><a class="category-list-link" href="/categories/programming-language/">programming language</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/python/">python</a><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/categories/python/matplotlib/">matplotlib</a></li></ul></li><li class="category-list-item"><a class="category-list-link" href="/categories/software-engineer/">software engineer</a><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/categories/software-engineer/API/">API</a></li></ul></li><li class="category-list-item"><a class="category-list-link" href="/categories/source-code/">source code</a><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/categories/source-code/JXIO/">JXIO</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/source-code/Spark/">Spark</a></li></ul></li><li class="category-list-item"><a class="category-list-link" href="/categories/spark/">spark</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/unity/">unity</a><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/categories/unity/animation/">animation</a></li></ul></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-star-o"> Tags</i></div><div class="tagcloud"></div></div><div class="widget"><div class="widget-title"><i class="fa fa-file-o"> Letzte</i></div><ul class="post-list"><li class="post-list-item"><a class="post-list-link" href="/2017/07/08/2017-07-08-unity-animation/">unity-animation</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/04/09/2017-04-09-dynamic-resource-allocation/">Dynamic Resource Allocation</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/03/06/2017-03-06-swprintf/">来聊一聊C++的string</a></li><li class="post-list-item"><a class="post-list-link" href="/2016/08/13/2016-08-13-beijing/">出去玩-北京</a></li><li class="post-list-item"><a class="post-list-link" href="/2016/05/31/2016-05-31-running-spark-on-mesos/">Running Spark on Mesos</a></li><li class="post-list-item"><a class="post-list-link" href="/2016/04/12/2016-04-12-spark-summary/">Spark小结</a></li><li class="post-list-item"><a class="post-list-link" href="/2016/03/25/2016-03-25-the-number-of-the-increasing-sub-sequences/">一个数组的递增子序列的个数</a></li><li class="post-list-item"><a class="post-list-link" href="/2015/12/13/2015-12-13-factorial-nonzero-suffix/">求N!的后9位不为0的数</a></li><li class="post-list-item"><a class="post-list-link" href="/2015/09/24/2015-09-24-recent-performance-improvements-in-apache-spark/">Recent Performance Improvements in Apache Spark: SQL, Python, DataFrame, and More</a></li><li class="post-list-item"><a class="post-list-link" href="/2015/08/15/2015-08-15-matplotlib/">Matplotlib整理</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-external-link"> Blogroll</i></div><ul></ul><a href="http://www.example1.com/" title="site-name1" target="_blank">site-name1</a><ul></ul><a href="http://www.example2.com/" title="site-name2" target="_blank">site-name2</a><ul></ul><a href="http://www.example3.com/" title="site-name3" target="_blank">site-name3</a></div></div></div><div class="pure-u-1 pure-u-md-3-4"><div id="footer">Copyright © 2017 <a href="/." rel="nofollow">ShallWe Talk.</a> Powered by<a rel="nofollow" target="_blank" href="https://hexo.io"> Hexo.</a><a rel="nofollow" target="_blank" href="https://github.com/tufu9441/maupassant-hexo"> Theme</a> by<a rel="nofollow" target="_blank" href="https://github.com/pagecho"> Cho.</a></div></div></div><a id="rocket" href="#top" class="show"></a><script type="text/javascript" src="/js/totop.js?v=0.0.0" async></script><script type="text/javascript" src="//cdn.bootcss.com/fancybox/3.1.20/jquery.fancybox.min.js" async></script><script type="text/javascript" src="/js/fancybox.js?v=0.0.0" async></script><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/fancybox/3.1.20/jquery.fancybox.min.css"><script type="text/javascript" src="/js/codeblock-resizer.js?v=0.0.0"></script><script type="text/javascript" src="/js/smartresize.js?v=0.0.0"></script></div></body></html>