<!DOCTYPE html><html lang="null"><head><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description"><title>Spark Execute | ShallWe Talk</title><link rel="stylesheet" type="text/css" href="/css/style.css?v=0.0.0"><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/normalize/7.0.0/normalize.min.css"><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/pure/1.0.0/pure-min.css"><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/pure/1.0.0/grids-responsive-min.css"><link rel="stylesheet" href="//cdn.bootcss.com/font-awesome/4.7.0/css/font-awesome.min.css"><script type="text/javascript" src="//cdn.bootcss.com/jquery/3.2.1/jquery.min.js"></script><link rel="Shortcut Icon" type="image/x-icon" href="/favicon.ico"><link rel="apple-touch-icon" href="/apple-touch-icon.png"><link rel="apple-touch-icon-precomposed" href="/apple-touch-icon.png"></head><body><div class="body_container"><div id="header"><div class="site-name"><h1 class="hidden">Spark Execute</h1><a id="logo" href="/.">ShallWe Talk</a><p class="description"></p></div><div id="nav-menu"><a href="/." class="current"><i class="fa fa-home"> Start</i></a><a href="/archives/"><i class="fa fa-archive"> Archiv</i></a><a href="/about/"><i class="fa fa-user"> Über</i></a><a href="/atom.xml"><i class="fa fa-rss"> RSS</i></a></div></div><div id="layout" class="pure-g"><div class="pure-u-1 pure-u-md-3-4"><div class="content_container"><div class="post"><h1 class="post-title">Spark Execute</h1><div class="post-meta">Jun 1, 2015<span> | </span><span class="category"><a href="/categories/source-code/">source code</a><a href="/categories/source-code/Spark/">Spark</a></span></div><div class="post-content"><p>今天是6月1号星期一，农历xxxx（艾玛我不知道）。传说中的小孩子过的一个节日。俗话说得好，每逢佳节倍思亲，等下去给娘亲打个电话就酱。  </p>
<p>今天也是每周开组会的日子，机智的我就在刚才已经想好这周组会讲什么了，于是可以淡定地继续写日志了。因为不能絮絮叨叨好好写一篇纯有感而发的日志，所以只能在每篇日志开头絮絮叨叨。  </p>
<p>顺便做个小结，一般小结都在最后做，但是因为太！重！要！所以开头说。这个任务执行的过程基本就是如下调用链，我们也会按照这个调用链往下讲：  </p>
<p><code>TaskRunner.run-&gt;Task.run-&gt;Task.runTask-&gt;RDD.iterator-&gt;RDD.computeOrReadCheckpoint-&gt;RDD.compute</code>  </p>
<p>好了，还是接着上次的Spark任务调度，现在要在每个Worker上执行task了。<a href="http://6carol6.github.io/blog/2015/05/22/spark-scheduling/" target="_blank" rel="external">上回</a>我们说到要在启动的Executor上执行任务了，就像这样：  </p>
<a id="more"></a>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">run</span></span>()&#123;</div><div class="line">	...</div><div class="line">	<span class="keyword">val</span> (taskFiles, taskJars, taskBytes) = <span class="type">Task</span>.deserializeWithDependencies(serializedTask)</div><div class="line">	updateDependencies(taskFiles, taskJars)</div><div class="line">	task = ser.deserialize[<span class="type">Task</span>[<span class="type">Any</span>]](taskBytes, <span class="type">Thread</span>.currentThread.getContextClassLoader)</div><div class="line">	...</div><div class="line">	<span class="keyword">val</span> value = task.run(taskId.toInt)</div><div class="line">	<span class="comment">//然后后面就是序列化结果传出来啊什么的，先放一下。</span></div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>我们先看一下<code>updateDependencies</code>做了什么事情。  </p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div></pre></td><td class="code"><pre><div class="line"><span class="comment">/**</span></div><div class="line"><span class="comment"> * Download any missing dependencies if we receive a new set of files and JARs from the</span></div><div class="line"><span class="comment"> * SparkContext. Also adds any new JARs we fetched to the class loader.</span></div><div class="line"><span class="comment">**/</span></div><div class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">updateDependencies</span></span>(newFiles: <span class="type">HashMap</span>[<span class="type">String</span>, <span class="type">Long</span>], newJars: <span class="type">HashMap</span>[<span class="type">String</span>, <span class="type">Long</span>]) &#123;</div><div class="line">  synchronized &#123;</div><div class="line">    <span class="comment">// Fetch missing dependencies</span></div><div class="line">    <span class="keyword">for</span> ((name, timestamp) &lt;- newFiles <span class="keyword">if</span> currentFiles.getOrElse(name, <span class="number">-1</span>L) &lt; timestamp) &#123;</div><div class="line">      logInfo(<span class="string">"Fetching "</span> + name + <span class="string">" with timestamp "</span> + timestamp)</div><div class="line">      <span class="type">Utils</span>.fetchFile(name, <span class="keyword">new</span> <span class="type">File</span>(<span class="type">SparkFiles</span>.getRootDirectory), conf, env.securityManager)</div><div class="line">      currentFiles(name) = timestamp</div><div class="line">    &#125;</div><div class="line">    <span class="keyword">for</span> ((name, timestamp) &lt;- newJars <span class="keyword">if</span> currentJars.getOrElse(name, <span class="number">-1</span>L) &lt; timestamp) &#123;</div><div class="line">      logInfo(<span class="string">"Fetching "</span> + name + <span class="string">" with timestamp "</span> + timestamp)</div><div class="line">      <span class="type">Utils</span>.fetchFile(name, <span class="keyword">new</span> <span class="type">File</span>(<span class="type">SparkFiles</span>.getRootDirectory), conf, env.securityManager)</div><div class="line">      currentJars(name) = timestamp</div><div class="line">      <span class="comment">// Add it to our class loader</span></div><div class="line">      <span class="keyword">val</span> localName = name.split(<span class="string">"/"</span>).last</div><div class="line">      <span class="keyword">val</span> url = <span class="keyword">new</span> <span class="type">File</span>(<span class="type">SparkFiles</span>.getRootDirectory, localName).toURI.toURL</div><div class="line">      <span class="keyword">if</span> (!urlClassLoader.getURLs.contains(url)) &#123;</div><div class="line">        logInfo(<span class="string">"Adding "</span> + url + <span class="string">" to class loader"</span>)</div><div class="line">        urlClassLoader.addURL(url)</div><div class="line">      &#125;</div><div class="line">    &#125;</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>注意到这里有一个时间戳的比较，如果当前本地的file或是jar包比依赖的file或jar的时间戳要旧，才会需要更新。  </p>
<p>首先是对file的更新。执行了<code>Utils.fetchFile(name, new File(SparkFiles.getRootDirectory), conf, env.securityManager)</code>，然后更新一下本地文件的时间戳。</p>
<p>然后是对jar的更新。前三行做了与更新file相同的事情，然后由于是jar包，需要添加到class loader中。</p>
<p>========================================<strong>关于Utils.fetchFile</strong>=========================================  </p>
<p>首先看注释：  </p>
<blockquote>
<p>Download a file requested by the executor. Supports fetching the file in a variety of ways,<br>including HTTP, HDFS and files on a standard filesystem, based on the URL parameter.</p>
<p>Throws SparkException if the target file already exists and has different contents than<br>the requested file.</p>
</blockquote>
<p>就是说根据URL的格式来判断是要读一个HTTP还是HDFS还是files on a standard filesystem。然后把这个文件获取出来。所以分了三个case，分别是”http|https|ftp”，”local file”，”HDFS file”。  </p>
<p>首先是<code>http|https|ftp</code>。就是建个连接，然后用<code>FileOutputStream</code>把文件读下来。然后是<code>local file</code>，就是执行<code>Files.copy(sourceFile, targetFile)</code>。最后是<code>HDFS file</code>，同样用<code>FileOutputStream</code>把文件下下来。不管是哪种方式都会判断这个文件是否已经存在，如果存在就不需要下载/复制了。  </p>
<p>文件读取结束之后，还会解压.tar.gz/.tgz/.tar等格式的文件，然后会赋给文件<code>a+x</code>权限。考虑得真周到です。  </p>
<p>========================================<strong>Utils.fetchFile结束</strong>=========================================  </p>
<p>更新完了这些依赖就开始<code>val value = task.run(taskId.toInt)</code>了。这个函数长这样：  </p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">final</span> <span class="function"><span class="keyword">def</span> <span class="title">run</span></span>(attemptId: <span class="type">Long</span>): <span class="type">T</span> = &#123;</div><div class="line">  context = <span class="keyword">new</span> <span class="type">TaskContext</span>(stageId, partitionId, attemptId, runningLocally = <span class="literal">false</span>)</div><div class="line">  taskThread = <span class="type">Thread</span>.currentThread()</div><div class="line">  <span class="keyword">if</span> (_killed) &#123;</div><div class="line">    kill(interruptThread = <span class="literal">false</span>)</div><div class="line">  &#125;</div><div class="line">  runTask(context)</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>首先new了一个<code>TaskContext</code>。这个东西可以设置<strong>task完成之后调用什么函数</strong>啊什么的，不过还么找到在哪设的。然后搞一个线程过来执行程序吧！这里注意到Task.scala其实是一个<code>abstract class</code>。我们需要找到实现这个class的类才能找到<code>runTask</code>的实现。在<a href="http://6carol6.github.io/blog/2015/05/22/spark-scheduling/" target="_blank" rel="external">Spark Scheduling</a>阶段的介绍里我们已经说过，Task总共有两种类型——ShuffleMapTask和ResultTask。最后一个Task是ResultTask，其余的都是ShuffleMapTask。我们分别来看这两种Task都是怎么run的。</p>
<p>###1 仅包含ResultTask的情况(简单得多)###</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">runTask</span></span>(context: <span class="type">TaskContext</span>): <span class="type">U</span> = &#123;</div><div class="line">  metrics = <span class="type">Some</span>(context.taskMetrics)</div><div class="line">  <span class="keyword">try</span> &#123;</div><div class="line">    func(context, rdd.iterator(split, context))</div><div class="line">  &#125; <span class="keyword">finally</span> &#123;</div><div class="line">    context.executeOnCompleteCallbacks()</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>就是简单调用一下func（在Master新建ResultTask的时候传入的），然后调用一下<code>context.executeOnCompleteCallbacks()</code>做一些额外的事情，然后结束。这个callback函数在某些RDD里面会定义。<code>比如HadoopRDD</code>。  </p>
<p>让我们回到最初的那个栗子，由于没有shuffle这个例子就是一个典型的ResultTask：  </p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">val</span> textFile = sc.textFile(<span class="string">"readme.md"</span>)</div><div class="line">textFile.filter(line=&gt;line.contains(<span class="string">"spark"</span>)).count()</div></pre></td></tr></table></figure>
<p>我们回顾一下在<a href="http://6carol6.github.io/blog/2015/05/22/spark-scheduling/" target="_blank" rel="external">Spark Scheduling</a>中把这两行代码转化成了一个ResultTask。看<code>count()</code>操作的定义可以知道，runTask中的func在这里就是<code>Utils.getIteratorSize _</code>看下注释大概意思就是说对iterator中的元素计数。然后<code>count()</code>操作会把这个结果在sum一下，得到所有iterator的元素个数的和，以实现最终的计数功能。  </p>
<p>对于RDD的计算是一个递归调用的过程。在最终的runTask里是<code>func(context, rdd.iterator(split, context))</code>，就递归到了<code>rdd.iterator</code>这里的rdd是FilteredRDD，我们从FilteredRDD的compute函数开始看。  </p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">compute</span></span>(split: <span class="type">Partition</span>, context: <span class="type">TaskContext</span>) =</div><div class="line">    firstParent[<span class="type">T</span>].iterator(split, context).filter(f)</div></pre></td></tr></table></figure>
<p>可以知道这里的f是<code>line=&gt;line.contains(&quot;spark&quot;)</code>。然后我们找到<code>RDD.firstParent</code>的定义：  </p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="comment">/** Returns the first parent RDD */</span></div><div class="line"><span class="keyword">protected</span>[spark] <span class="function"><span class="keyword">def</span> <span class="title">firstParent</span></span>[<span class="type">U</span>: <span class="type">ClassTag</span>] = &#123;</div><div class="line">  dependencies.head.rdd.asInstanceOf[<span class="type">RDD</span>[<span class="type">U</span>]]</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>所以小结一下这里就是先执行<code>MappedRDD.compute</code>然后再对计算结果进行<code>.filter(f)</code>，这个<code>f</code>就是<code>line=&gt;line.contains(spark)</code>。ok，搞清楚了就看<code>MappedRDD.compute</code>  </p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">compute</span></span>(split: <span class="type">Partition</span>, context: <span class="type">TaskContext</span>) =</div><div class="line">  firstParent[<span class="type">T</span>].iterator(split, context).map(f)</div></pre></td></tr></table></figure>
<p>那么这个<code>f</code>又是啥？在<code>SparkContext.map()</code>里可以找到答案是<code>pair =&gt; pair._2.toString</code>。OK于是往上遍历到了<code>HadoopRDD</code>。  </p>
<p>我们在<a href="http://6carol6.github.io/blog/2015/06/03/start-from-hadooprdd/" target="_blank" rel="external">这篇文章</a>中说到：HadoopRDD的<code>compute()</code>就是提供了一个对HDFS中数据的迭代器。  </p>
<p>所以这里就是对每个split的<key, value="">进行pair=&gt;pair._2.toString送到下一步。也就是说，现在对所有<key, value="">都只取value，并转换成String。  </key,></key,></p>
<p>所以到这里我们已经看完了所有计算过程：首先用HadoopRDD得到每个<key, value="">的迭代器，然后进行map操作，保留value，生成MapRDD，然后用filter操作过滤掉不含Spark的行，生成FilteredRDD，最后对每个iterator的结果进行求和，至此runJob结束，再在master上对所有iterator的结果进行求和(sum)。  </key,></p>
<p>至此完成了所有任务。  </p>
<p>###2 包含ShuffleMapTask的情况###</p>
<p>同样的，我们来看一下ShuffleMapTask的runTask：  </p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">runTask</span></span>(context: <span class="type">TaskContext</span>): <span class="type">MapStatus</span> = &#123;</div><div class="line">  <span class="keyword">val</span> numOutputSplits = dep.partitioner.numPartitions</div><div class="line">  metrics = <span class="type">Some</span>(context.taskMetrics)</div><div class="line"></div><div class="line">  <span class="keyword">val</span> blockManager = <span class="type">SparkEnv</span>.get.blockManager</div><div class="line">  <span class="keyword">val</span> shuffleBlockManager = blockManager.shuffleBlockManager</div><div class="line">  <span class="keyword">var</span> shuffle: <span class="type">ShuffleWriterGroup</span> = <span class="literal">null</span></div><div class="line">  <span class="keyword">var</span> success = <span class="literal">false</span></div><div class="line">	</div><div class="line">  <span class="keyword">try</span> &#123;</div><div class="line">    <span class="comment">// Obtain all the block writers for shuffle blocks.</span></div><div class="line">    <span class="keyword">val</span> ser = <span class="type">Serializer</span>.getSerializer(dep.serializer)</div><div class="line">	shuffle = shuffleBlockManager.forMapTask(dep.shuffleId, partitionId, numOutputSplits, ser)</div><div class="line">	</div><div class="line">    <span class="comment">// Write the map output to its associated buckets.</span></div><div class="line">	<span class="comment">//就在这里调用rdd.iterator执行啦~~~所以暂停看这句。</span></div><div class="line">    <span class="keyword">for</span> (elem &lt;- rdd.iterator(split, context)) &#123;</div><div class="line">	   <span class="keyword">val</span> pair = elem.asInstanceOf[<span class="type">Product2</span>[<span class="type">Any</span>, <span class="type">Any</span>]]</div><div class="line">	   <span class="keyword">val</span> bucketId = dep.partitioner.getPartition(pair._1)</div><div class="line">	   shuffle.writers(bucketId).write(pair)</div><div class="line">	&#125;</div><div class="line">	</div><div class="line">	<span class="comment">// Commit the writes. Get the size of each bucket block (total block size).</span></div><div class="line">	<span class="keyword">var</span> totalBytes = <span class="number">0</span>L</div><div class="line">	<span class="keyword">var</span> totalTime = <span class="number">0</span>L</div><div class="line">	<span class="keyword">val</span> compressedSizes: <span class="type">Array</span>[<span class="type">Byte</span>] = shuffle.writers.map &#123; writer: <span class="type">BlockObjectWriter</span> =&gt;</div><div class="line">	  writer.commit()</div><div class="line">	  writer.close()</div><div class="line">	  <span class="keyword">val</span> size = writer.fileSegment().length</div><div class="line">	  totalBytes += size</div><div class="line">	  totalTime += writer.timeWriting()</div><div class="line">	  <span class="type">MapOutputTracker</span>.compressSize(size)</div><div class="line">    &#125;</div><div class="line">	</div><div class="line">    <span class="comment">// Update shuffle metrics.</span></div><div class="line">    <span class="keyword">val</span> shuffleMetrics = <span class="keyword">new</span> <span class="type">ShuffleWriteMetrics</span></div><div class="line">    shuffleMetrics.shuffleBytesWritten = totalBytes</div><div class="line">    shuffleMetrics.shuffleWriteTime = totalTime</div><div class="line">    metrics.get.shuffleWriteMetrics = <span class="type">Some</span>(shuffleMetrics)</div><div class="line">	</div><div class="line">    success = <span class="literal">true</span></div><div class="line">    <span class="keyword">new</span> <span class="type">MapStatus</span>(blockManager.blockManagerId, compressedSizes)</div><div class="line">  &#125; <span class="keyword">catch</span> &#123; <span class="keyword">case</span> e: <span class="type">Exception</span> =&gt;</div><div class="line">    <span class="comment">// If there is an exception from running the task, revert the partial writes</span></div><div class="line">	<span class="comment">// and throw the exception upstream to Spark.</span></div><div class="line">	<span class="keyword">if</span> (shuffle != <span class="literal">null</span> &amp;&amp; shuffle.writers != <span class="literal">null</span>) &#123;</div><div class="line">	  <span class="keyword">for</span> (writer &lt;- shuffle.writers) &#123;</div><div class="line">		writer.revertPartialWrites()</div><div class="line">		writer.close()</div><div class="line">	  &#125;</div><div class="line">	&#125;</div><div class="line">	<span class="keyword">throw</span> e</div><div class="line">  &#125; <span class="keyword">finally</span> &#123;</div><div class="line">	<span class="comment">// Release the writers back to the shuffle block manager.</span></div><div class="line">	<span class="keyword">if</span> (shuffle != <span class="literal">null</span> &amp;&amp; shuffle.writers != <span class="literal">null</span>) &#123;</div><div class="line">	  <span class="keyword">try</span> &#123;</div><div class="line">		shuffle.releaseWriters(success)</div><div class="line">	  &#125; <span class="keyword">catch</span> &#123;</div><div class="line">		<span class="keyword">case</span> e: <span class="type">Exception</span> =&gt; logError(<span class="string">"Failed to release shuffle writers"</span>, e)</div><div class="line">	  &#125;</div><div class="line">	&#125;</div><div class="line">	<span class="comment">// Execute the callbacks on task completion.</span></div><div class="line">	context.executeOnCompleteCallbacks()</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>首先要获取<code>BlockManager</code>。这里要注意，在整个Spark的执行过程当中，<strong>只有<code>SparkEnv.scala</code>中有一个<code>new BlockManager</code></strong>。因此，在所有需要获取数据块的地方，只能从<code>SparkEnv.get.BlockManager</code>拿到<code>BlockManager</code>。然后获取了一个<code>ShuffleBlockManager</code>。注意在当初<code>new ShffleBlockManager</code>的时候传入的参数就是<code>BlockManager.this</code>呢。然后要获取所有block的writer，把每个map output与它们的bucket联系起来。说到bucket，我们可以看一下spark shuffle的过程图。  </p>
<p>我们可以看一下spark shuffle的过程：  </p>
<p><img src="https://6carol6.github.com/images/spark-shuffle.png" alt="spark shuffle"></p>
<p>每一个map task(这就是为什么这个Task叫ShuffleMapTask)结束之后都要把文件写到对应的bucket里，便于reduce来获取。然后就默默调用<code>RDD.iterator</code>执行啦。  </p>
<p>所以来看一下<code>RDD.iterator</code>:  </p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line"><span class="comment">/**</span></div><div class="line"><span class="comment">  * Internal method to this RDD; will read from cache if applicable, or otherwise compute it.</span></div><div class="line"><span class="comment">  * This should ''not'' be called by users directly, but is available for implementors of custom</span></div><div class="line"><span class="comment">  * subclasses of RDD.</span></div><div class="line"><span class="comment">*/</span></div><div class="line"><span class="keyword">final</span> <span class="function"><span class="keyword">def</span> <span class="title">iterator</span></span>(split: <span class="type">Partition</span>, context: <span class="type">TaskContext</span>): <span class="type">Iterator</span>[<span class="type">T</span>] = &#123;</div><div class="line">  <span class="keyword">if</span> (storageLevel != <span class="type">StorageLevel</span>.<span class="type">NONE</span>) &#123;</div><div class="line">    <span class="type">SparkEnv</span>.get.cacheManager.getOrCompute(<span class="keyword">this</span>, split, context, storageLevel)</div><div class="line">  &#125; <span class="keyword">else</span> &#123;</div><div class="line">	computeOrReadCheckpoint(split, context)</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>这里很容易看就先判断一下是不是已经cache了啊，我们这里就假设么被cache，直接跳到<code>RDD.computeOrReadCheckpoint</code>。  </p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="comment">/**</span></div><div class="line"><span class="comment">  * Compute an RDD partition or read it from a checkpoint if the RDD is checkpointing.</span></div><div class="line"><span class="comment">*/</span></div><div class="line"><span class="keyword">private</span>[spark] <span class="function"><span class="keyword">def</span> <span class="title">computeOrReadCheckpoint</span></span>(split: <span class="type">Partition</span>, context: <span class="type">TaskContext</span>): <span class="type">Iterator</span>[<span class="type">T</span>] =</div><div class="line">&#123;</div><div class="line">  <span class="keyword">if</span> (isCheckpointed) firstParent[<span class="type">T</span>].iterator(split, context) <span class="keyword">else</span> compute(split, context)</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>如果么得checkpoint，直接调用<code>RDD.compute()</code>。这个东西就仁者见仁智者见智了，不同的RDD都由不同的<code>compute()</code>。如果没有猜错，应该是从这个task的最后一个RDD开始啦。  </p>
<p>现在我们来举一个复杂一点的栗子（一个带Shuffle的栗子）：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">val</span> textFile = sc.textFile(<span class="string">"README.md"</span>)</div><div class="line">textFile.flatMap(line =&gt; line.split(<span class="string">" "</span>)).map(word =&gt; (word, <span class="number">1</span>)).reduceByKey((a, b) =&gt; a + b)</div></pre></td></tr></table></figure>
<p>看到这里，我们应该很熟悉textFile操作了，会生成一个HadoopRDD然后再通过map操作变成MappedRDD。所以从flatMap看起就可以了：  </p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="comment">/**</span></div><div class="line"><span class="comment">*  Return a new RDD by first applying a function to all elements of this</span></div><div class="line"><span class="comment">*  RDD, and then flattening the results.</span></div><div class="line"><span class="comment">*/</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">flatMap</span></span>[<span class="type">U</span>: <span class="type">ClassTag</span>](f: <span class="type">T</span> =&gt; <span class="type">TraversableOnce</span>[<span class="type">U</span>]): <span class="type">RDD</span>[<span class="type">U</span>] =</div><div class="line">  <span class="keyword">new</span> <span class="type">FlatMappedRDD</span>(<span class="keyword">this</span>, sc.clean(f))</div></pre></td></tr></table></figure>
<p>然后又生成了一个MappedRDD。由于MappedRDD是<key, value="">类型的RDD，所以可以隐式转换成<code>PairRDDFunctions</code>，执行<code>reduceByKey</code>操作。（只有<key, value="">类型的RDD才可以隐式转换然后用这些shuffle方法，我们在<a href="http://6carol6.github.io/blog/2015/04/17/rdd-in-spark/" target="_blank" rel="external">这篇文章</a>中已经说明过了）  </key,></key,></p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div></pre></td><td class="code"><pre><div class="line"><span class="comment">/**</span></div><div class="line"><span class="comment">* Merge the values for each key using an associative reduce function. This will also perform</span></div><div class="line"><span class="comment">* the merging locally on each mapper before sending results to a reducer, similarly to a</span></div><div class="line"><span class="comment">* "combiner" in MapReduce.</span></div><div class="line"><span class="comment">*/</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">reduceByKey</span></span>(partitioner: <span class="type">Partitioner</span>, func: (<span class="type">V</span>, <span class="type">V</span>) =&gt; <span class="type">V</span>): <span class="type">RDD</span>[(<span class="type">K</span>, <span class="type">V</span>)] = &#123;</div><div class="line">  combineByKey[<span class="type">V</span>]((v: <span class="type">V</span>) =&gt; v, func, func, partitioner)</div><div class="line">&#125;</div><div class="line">	</div><div class="line"><span class="comment">/**</span></div><div class="line"><span class="comment">* Simplified version of combineByKey that hash-partitions the output RDD.</span></div><div class="line"><span class="comment">*/</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">combineByKey</span></span>[<span class="type">C</span>](createCombiner: <span class="type">V</span> =&gt; <span class="type">C</span>,</div><div class="line">  mergeValue: (<span class="type">C</span>, <span class="type">V</span>) =&gt; <span class="type">C</span>,</div><div class="line">  mergeCombiners: (<span class="type">C</span>, <span class="type">C</span>) =&gt; <span class="type">C</span>,</div><div class="line">  numPartitions: <span class="type">Int</span>): <span class="type">RDD</span>[(<span class="type">K</span>, <span class="type">C</span>)] = &#123;</div><div class="line">  combineByKey(createCombiner, mergeValue, mergeCombiners, <span class="keyword">new</span> <span class="type">HashPartitioner</span>(numPartitions))</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="comment">/**</span></div><div class="line"><span class="comment">* Generic function to combine the elements for each key using a custom set of aggregation</span></div><div class="line"><span class="comment">* functions. Turns an RDD[(K, V)] into a result of type RDD[(K, C)], for a "combined type" C</span></div><div class="line"><span class="comment">* Note that V and C can be different -- for example, one might group an RDD of type</span></div><div class="line"><span class="comment">* (Int, Int) into an RDD of type (Int, Seq[Int]). Users provide three functions:</span></div><div class="line"><span class="comment">*</span></div><div class="line"><span class="comment">* - `createCombiner`, which turns a V into a C (e.g., creates a one-element list)</span></div><div class="line"><span class="comment">* - `mergeValue`, to merge a V into a C (e.g., adds it to the end of a list)</span></div><div class="line"><span class="comment">* - `mergeCombiners`, to combine two C's into a single one.</span></div><div class="line"><span class="comment">*</span></div><div class="line"><span class="comment">* In addition, users can control the partitioning of the output RDD, and whether to perform</span></div><div class="line"><span class="comment">* map-side aggregation (if a mapper can produce multiple items with the same key).</span></div><div class="line"><span class="comment">*/</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">combineByKey</span></span>[<span class="type">C</span>](createCombiner: <span class="type">V</span> =&gt; <span class="type">C</span>,</div><div class="line">  mergeValue: (<span class="type">C</span>, <span class="type">V</span>) =&gt; <span class="type">C</span>, <span class="comment">//mergeValue和mergeCombiner都是(a,b)=&gt;a+b</span></div><div class="line">  mergeCombiners: (<span class="type">C</span>, <span class="type">C</span>) =&gt; <span class="type">C</span>,</div><div class="line">  partitioner: <span class="type">Partitioner</span>,</div><div class="line">  mapSideCombine: <span class="type">Boolean</span> = <span class="literal">true</span>,</div><div class="line">  serializer: <span class="type">Serializer</span> = <span class="literal">null</span>): <span class="type">RDD</span>[(<span class="type">K</span>, <span class="type">C</span>)] = &#123;</div><div class="line">    require(mergeCombiners != <span class="literal">null</span>, <span class="string">"mergeCombiners must be defined"</span>) <span class="comment">// required as of Spark 0.9.0</span></div><div class="line">    <span class="keyword">if</span> (keyClass.isArray) &#123;</div><div class="line">      <span class="comment">//如果key是array，就不能做map-side combining，为啥捏？</span></div><div class="line">	  <span class="keyword">if</span> (mapSideCombine) &#123;</div><div class="line">	    <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">SparkException</span>(<span class="string">"Cannot use map-side combining with array keys."</span>)</div><div class="line">	  &#125;</div><div class="line">	  <span class="keyword">if</span> (partitioner.isInstanceOf[<span class="type">HashPartitioner</span>]) &#123;</div><div class="line">	  <span class="comment">//这里要求partitioner不能是HashPartitioner类型（如果key是array）</span></div><div class="line">	    <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">SparkException</span>(<span class="string">"Default partitioner cannot partition array keys."</span>)</div><div class="line">	  &#125;</div><div class="line">	&#125;</div><div class="line">	<span class="keyword">val</span> aggregator = <span class="keyword">new</span> <span class="type">Aggregator</span>[<span class="type">K</span>, <span class="type">V</span>, <span class="type">C</span>](createCombiner, mergeValue, mergeCombiners)</div><div class="line">	<span class="keyword">if</span> (self.partitioner == <span class="type">Some</span>(partitioner)) &#123; <span class="comment">//这是什么情况？</span></div><div class="line">	  self.mapPartitionsWithContext((context, iter) =&gt; &#123;</div><div class="line">		<span class="keyword">new</span> <span class="type">InterruptibleIterator</span>(context, aggregator.combineValuesByKey(iter, context))</div><div class="line">	  &#125;, preservesPartitioning = <span class="literal">true</span>)</div><div class="line">	&#125; <span class="keyword">else</span> <span class="keyword">if</span> (mapSideCombine) &#123;</div><div class="line">	  <span class="comment">//如果是map端的combine是要new ShuffleRDD的，这个值默认是true的。</span></div><div class="line">	  <span class="keyword">val</span> combined = self.mapPartitionsWithContext((context, iter) =&gt; &#123;</div><div class="line">		aggregator.combineValuesByKey(iter, context)</div><div class="line">	  &#125;, preservesPartitioning = <span class="literal">true</span>)</div><div class="line">	  <span class="keyword">val</span> partitioned = <span class="keyword">new</span> <span class="type">ShuffledRDD</span>[<span class="type">K</span>, <span class="type">C</span>, (<span class="type">K</span>, <span class="type">C</span>)](combined, partitioner)</div><div class="line">		.setSerializer(serializer)</div><div class="line">		partitioned.mapPartitionsWithContext((context, iter) =&gt; &#123;</div><div class="line">		  <span class="keyword">new</span> <span class="type">InterruptibleIterator</span>(context, aggregator.combineCombinersByKey(iter, context))</div><div class="line">		&#125;, preservesPartitioning = <span class="literal">true</span>)</div><div class="line">	&#125; <span class="keyword">else</span> &#123;</div><div class="line">	  <span class="comment">// Don't apply map-side combiner.</span></div><div class="line">	  <span class="comment">//如果不要求进行mapSideCombine</span></div><div class="line">	  <span class="keyword">val</span> values = <span class="keyword">new</span> <span class="type">ShuffledRDD</span>[<span class="type">K</span>, <span class="type">V</span>, (<span class="type">K</span>, <span class="type">V</span>)](self, partitioner).setSerializer(serializer)</div><div class="line">	  values.mapPartitionsWithContext((context, iter) =&gt; &#123;</div><div class="line">		<span class="keyword">new</span> <span class="type">InterruptibleIterator</span>(context, aggregator.combineValuesByKey(iter, context))</div><div class="line">	  &#125;, preservesPartitioning = <span class="literal">true</span>)</div><div class="line">	&#125;</div><div class="line">  &#125;</div></pre></td></tr></table></figure>
<p>可以看到mapSideCombine这种情况首先combine了一下，然后new ShuffledRDD，然后再combine一下。其它两种情况都是这种情况的子情况。ok，首先combine了一下使用了  </p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">self.mapPartitionsWithContext((context, iter)=&gt;&#123;</div><div class="line">	aggregator.combineValuesByKey(iter, context)</div><div class="line">&#125;, preservesPartitioning = <span class="literal">true</span>)</div></pre></td></tr></table></figure>
<p>RDD.scala中<code>mapPartitionsWithContext</code>方法如下：  </p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line"><span class="comment">/**</span></div><div class="line"><span class="comment">* :: DeveloperApi ::</span></div><div class="line"><span class="comment">* Return a new RDD by applying a function to each partition of this RDD. This is a variant of</span></div><div class="line"><span class="comment">* mapPartitions that also passes the TaskContext into the closure.</span></div><div class="line"><span class="comment">*/</span></div><div class="line"><span class="meta">@DeveloperApi</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">mapPartitionsWithContext</span></span>[<span class="type">U</span>: <span class="type">ClassTag</span>](</div><div class="line">  f: (<span class="type">TaskContext</span>, <span class="type">Iterator</span>[<span class="type">T</span>]) =&gt; <span class="type">Iterator</span>[<span class="type">U</span>],</div><div class="line">  preservesPartitioning: <span class="type">Boolean</span> = <span class="literal">false</span>): <span class="type">RDD</span>[<span class="type">U</span>] = &#123;</div><div class="line">	<span class="keyword">val</span> func = (context: <span class="type">TaskContext</span>, index: <span class="type">Int</span>, iter: <span class="type">Iterator</span>[<span class="type">T</span>]) =&gt; f(context, iter)</div><div class="line">	  <span class="keyword">new</span> <span class="type">MapPartitionsRDD</span>(<span class="keyword">this</span>, sc.clean(func), preservesPartitioning)</div><div class="line">	&#125;</div></pre></td></tr></table></figure>
<p>然后new了一个ShuffledRDD，对这个RDD同样执行了上述方法。  </p>
<p><strong>这里要注意new ShuffledRDD的第一个参数传入的是上一个RDD，这对后面ShuffledRDD.getDependencies造成了很大的影响。以至于后面的ShuffleDependency.getRDD其实得到的是ShuffledRDD的上一个RDD。</strong></p>
<p>也就是说reduceByKey分了三个阶段：  </p>
<ul>
<li>转化成MapPartitionsRDD</li>
<li>转化成ShuffledRDD</li>
<li>转化成MapPartitionsRDD</li>
</ul>
<p>这事情非常符合我们的认知，就是在Map阶段会进行一次combine（其实就是reduce），然后shuffle，然后reduce。  </p>
<p>OK，现在问题就转到了我们关心的partition是如何划分的？Shuffle是如何获取数据的？这些问题。  </p>
<p>在<a href="http://6carol6.github.io/blog/2015/05/22/spark-scheduling/" target="_blank" rel="external">spark scheduling</a>中我们已经说过Spark会根据宽窄依赖来划分Stage，不同的Stage就有不同的task。ShuffledRDD之前的部分会被划分到ShuffleMapTask，而之后的部分会划分到ResultTask。这里我们从ShuffleMapTask看起。  </p>
<p>####2.1 ShuffleMapTask####</p>
<p>在这个栗子中，这个阶段有</p>
<blockquote>
<p>HadoopRDD-&gt;MappedRDD-&gt;FlatMappedRDD-&gt;MappedRDD-&gt;MapPartitionsRDD  </p>
</blockquote>
<p>前两个RDD的产生是由于textFile操作，我们已经很熟悉了。FlatMap的意思就是把一个<key, iter[]="">变成<key, iter[0]="">, <key, iter[1]="">, <key, iter[2]="">…在这个栗子里就是把文本划分出一个一个单词。然后又通过map，使得每个单词的计数为1，即变为<iter[0], 1="">, <iter[1], 1="">, <iter[2], 1="">。然后在reduceByKey阶段生成了MapPartitionsRDD，这才是我们要研究的重点。  </iter[2],></iter[1],></iter[0],></key,></key,></key,></key,></p>
<p>这个转换发生在combineByKey函数中。  </p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">val</span> combined = self.mapPartitionsWithContext((context, iter) =&gt; &#123;</div><div class="line">  aggregator.combineValuesByKey(iter, context)</div><div class="line">&#125;, preservesPartitioning = <span class="literal">true</span>)</div></pre></td></tr></table></figure>
<p>这里的self就是刚才的MappedRDD本身。根据之前的说明，可以看到这里会生成一个<code>MapPartitionsRDD</code>，我们就从这个RDD的compute函数开始看起。  </p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">compute</span></span>(split: <span class="type">Partition</span>, context: <span class="type">TaskContext</span>) =</div><div class="line">  f(context, split.index, firstParent[<span class="type">T</span>].iterator(split, context))</div></pre></td></tr></table></figure>
<p>这里的f是(context: TaskContext, index: Int, iter: Iterator[T]) =&gt; f(context, iter)，再往上看就是：  </p>
<pre><code>(context, iter)=&gt;{aggregator.combineValuesByKey(iter, context)}
</code></pre><p>Aggregator.scala  </p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">combineValuesByKey</span></span>(iter: <span class="type">Iterator</span>[_ &lt;: <span class="type">Product2</span>[<span class="type">K</span>, <span class="type">V</span>]],</div><div class="line">                       context: <span class="type">TaskContext</span>): <span class="type">Iterator</span>[(<span class="type">K</span>, <span class="type">C</span>)] = &#123;</div><div class="line">  <span class="keyword">if</span> (!externalSorting) &#123;</div><div class="line">    <span class="keyword">val</span> combiners = <span class="keyword">new</span> <span class="type">AppendOnlyMap</span>[<span class="type">K</span>,<span class="type">C</span>]</div><div class="line">    <span class="keyword">var</span> kv: <span class="type">Product2</span>[<span class="type">K</span>, <span class="type">V</span>] = <span class="literal">null</span></div><div class="line">    <span class="keyword">val</span> update = (hadValue: <span class="type">Boolean</span>, oldValue: <span class="type">C</span>) =&gt; &#123;</div><div class="line">      <span class="keyword">if</span> (hadValue) mergeValue(oldValue, kv._2) <span class="keyword">else</span> createCombiner(kv._2)</div><div class="line">    &#125;</div><div class="line">    <span class="keyword">while</span> (iter.hasNext) &#123;</div><div class="line">      kv = iter.next()</div><div class="line">      combiners.changeValue(kv._1, update)</div><div class="line">    &#125;</div><div class="line">    combiners.iterator</div><div class="line">  &#125; <span class="keyword">else</span> &#123;</div><div class="line">    <span class="keyword">val</span> combiners = <span class="keyword">new</span> <span class="type">ExternalAppendOnlyMap</span>[<span class="type">K</span>, <span class="type">V</span>, <span class="type">C</span>](createCombiner, mergeValue, mergeCombiners)</div><div class="line">    <span class="keyword">while</span> (iter.hasNext) &#123;</div><div class="line">      <span class="keyword">val</span> (k, v) = iter.next()</div><div class="line">      combiners.insert(k, v)</div><div class="line">    &#125;</div><div class="line">    <span class="comment">// <span class="doctag">TODO:</span> Make this non optional in a future release</span></div><div class="line">    <span class="type">Option</span>(context).foreach(c =&gt; c.taskMetrics.memoryBytesSpilled = combiners.memoryBytesSpilled)</div><div class="line">    <span class="type">Option</span>(context).foreach(c =&gt; c.taskMetrics.diskBytesSpilled = combiners.diskBytesSpilled)</div><div class="line">      combiners.iterator</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>这个函数就是返回一个combine之后的<key, value="">的迭代器。  </key,></p>
<p>得到了map-side combine后的迭代器之后，我们回到ShuffleMapTask的runTask。  </p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="comment">// Write the map output to its associated buckets.</span></div><div class="line"><span class="keyword">for</span> (elem &lt;- rdd.iterator(split, context)) &#123;</div><div class="line">  <span class="keyword">val</span> pair = elem.asInstanceOf[<span class="type">Product2</span>[<span class="type">Any</span>, <span class="type">Any</span>]]</div><div class="line">  <span class="keyword">val</span> bucketId = dep.partitioner.getPartition(pair._1)</div><div class="line">  shuffle.writers(bucketId).write(pair)</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>scala的官方文档说:  </p>
<blockquote>
<p>Product2 is a cartesian product of 2 components.</p>
</blockquote>
<p>所以pair就是一个笛卡尔积的形式。然后通过pair中的第一个分量，也就是单词，得到bucketId，并把这个pair写到对应的bucket里。  </p>
<p><strong>我们已经说过，ShuffledRDD实际上是把ShuffleDependency给上一个RDD，因此这里的dep.partitioner.getPartition得到的是ShuffledRDDPartition</strong>  </p>
<p>总之就是把这个Task的结果写到bucket里了，下一阶段来读就是了╮(╯▽╰)╭至于是怎么写的，我们将在<a href="">Spark BlockManager</a>给出详细的解释。  </p>
<p>####2.2 ResultTask####</p>
<p>这个阶段有</p>
<blockquote>
<p>ShuffledRDD-&gt;MapPartitionsRDD</p>
</blockquote>
<p>我们去看一下ShuffledRDD的compute函数：  </p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">compute</span></span>(split: <span class="type">Partition</span>, context: <span class="type">TaskContext</span>): <span class="type">Iterator</span>[<span class="type">P</span>] = &#123;</div><div class="line">  <span class="keyword">val</span> shuffledId = dependencies.head.asInstanceOf[<span class="type">ShuffleDependency</span>[<span class="type">K</span>, <span class="type">V</span>]].shuffleId</div><div class="line">  <span class="keyword">val</span> ser = <span class="type">Serializer</span>.getSerializer(serializer)</div><div class="line">  <span class="type">SparkEnv</span>.get.shuffleFetcher.fetch[<span class="type">P</span>](shuffledId, split.index, context, ser)</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>注意到这里有一个shuffleFetcher，通过(shuffledId, split.index)去远端获取数据了。感觉快要看到我们关心的地方了。  </p>
<p>这个ShffleFetcher其实是一个abstract的类，它的子类是<code>BlockStoreShuffleFetcher</code>。现在来看一下这个子类的<code>fetch</code>方法。  </p>
<p>========================<strong>关于shuffledId和split.index</strong>====================  </p>
<p>####1 ShuffledId####</p>
<p>早在<a href="http://6carol6.github.io/blog/2015/05/22/spark-scheduling/" target="_blank" rel="external">spark scheduling</a>这篇文章里讲<code>getMissingParentStages</code>的时候，有一个遍历<code>rdd.dependencies</code>的语句。这句话会调用各个rdd override的<code>getDependencies</code>，我们来看一下ShuffledRDD的<code>getDependencies</code>（<strong>这个函数只会被调用一次</strong>）  </p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">getDependencies</span></span>: <span class="type">Seq</span>[<span class="type">Dependency</span>[_]] = &#123;</div><div class="line">  <span class="type">List</span>(<span class="keyword">new</span> <span class="type">ShuffleDependency</span>(prev, part, serializer))</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>再看一下Dependency.scala  </p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div></pre></td><td class="code"><pre><div class="line"><span class="comment">/**</span></div><div class="line"><span class="comment"> * :: DeveloperApi ::</span></div><div class="line"><span class="comment"> * Represents a dependency on the output of a shuffle stage.</span></div><div class="line"><span class="comment"> * @param rdd the parent RDD</span></div><div class="line"><span class="comment"> * @param partitioner partitioner used to partition the shuffle output</span></div><div class="line"><span class="comment"> * @param serializer [[org.apache.spark.serializer.Serializer Serializer]] to use. If set to null,</span></div><div class="line"><span class="comment"> *                   the default serializer, as specified by `spark.serializer` config option, will</span></div><div class="line"><span class="comment"> *                   be used.</span></div><div class="line"><span class="comment"> */</span></div><div class="line"><span class="meta">@DeveloperApi</span></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">ShuffleDependency</span>[<span class="type">K</span>, <span class="type">V</span>](<span class="params"></span></span></div><div class="line"><span class="class"><span class="params">    @transient rdd: <span class="type">RDD</span>[_ &lt;: <span class="type">Product2</span>[<span class="type">K</span>, <span class="type">V</span>]],</span></span></div><div class="line"><span class="class"><span class="params">    val partitioner: <span class="type">Partitioner</span>,</span></span></div><div class="line"><span class="class"><span class="params">    val serializer: <span class="type">Serializer</span> = null</span>)</span></div><div class="line"><span class="class">  <span class="keyword">extends</span> <span class="title">Dependency</span>(<span class="params">rdd.asInstanceOf[<span class="type">RDD</span>[<span class="type">Product2</span>[<span class="type">K</span>, <span class="type">V</span>]]]</span>) </span>&#123;</div><div class="line"></div><div class="line">  <span class="keyword">val</span> shuffleId: <span class="type">Int</span> = rdd.context.newShuffleId()</div><div class="line"></div><div class="line">  rdd.sparkContext.cleaner.foreach(_.registerShuffleForCleanup(<span class="keyword">this</span>))</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>这里的rdd.context是一个SparkContext，rdd.context.newShuffleId其实就是在上一个shuffleId的基础上自增1而已。  </p>
<p>至此我们已经知道了<strong>ShuffledId是在划分stage的时候就分配了的，值是每次自增1的</strong>。</p>
<p>####2 split.index####</p>
<p>这里的split来自iterator传进的参数，这个iterator往上追溯是在ShuffleMapTask新建的时候给赋值的。  </p>
<p>ShuffleMapTask.scala  </p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">var</span> split = <span class="keyword">if</span> (rdd == <span class="literal">null</span>) <span class="literal">null</span> <span class="keyword">else</span> rdd.partitions(partitionId)</div></pre></td></tr></table></figure>
<p>rdd.partitions(partitionId)得到的是一个Partition，这个Partition的index又是从哪来的呢？  </p>
<p>我们在<a href="http://6carol6.github.io/blog/2015/06/03/start-from-hadooprdd/" target="_blank" rel="external">start from HadoopRDD</a>中曾经说过，当<code>rdd.partitions</code>第一次被调用的时候，就会调用这个RDD override的<code>getPartitions</code>。<strong>这里我们假设就是第一次调用吧（因为再也找不到上层了）</strong>于是就有了这个结果：  </p>
<p>ShufffledRDD.scala  </p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">getPartitions</span></span>: <span class="type">Array</span>[<span class="type">Partition</span>] = &#123;</div><div class="line">  <span class="type">Array</span>.tabulate[<span class="type">Partition</span>](part.numPartitions)(i =&gt; <span class="keyword">new</span> <span class="type">ShuffledRDDPartition</span>(i))</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>所以说其实ShuffledRDD有其独特的Partition。  </p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">private</span>[spark] <span class="class"><span class="keyword">class</span> <span class="title">ShuffledRDDPartition</span>(<span class="params">val idx: <span class="type">Int</span></span>) <span class="keyword">extends</span> <span class="title">Partition</span> </span>&#123;</div><div class="line">  <span class="keyword">override</span> <span class="keyword">val</span> index = idx <span class="comment">//这就是split.index</span></div><div class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">hashCode</span></span>(): <span class="type">Int</span> = idx</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>所以split.index就是那个i。就是从0到part.numPartitions的i。  </p>
<p>==========================<strong>解释结束</strong>==========================  </p>
</div><div class="tags"></div><div class="post-nav"><a href="/2015/06/03/start-from-hadooprdd/" class="pre">Start from HadoopRDD</a><a href="/2015/05/27/mapreduce-review/" class="next">MapReduce Review</a></div></div></div></div><div class="pure-u-1-4 hidden_mid_and_down"><div id="sidebar"><div class="widget"><form action="//www.google.com/search" method="get" accept-charset="utf-8" target="_blank" class="search-form"><input type="text" name="q" maxlength="20" placeholder="Search"/><input type="hidden" name="sitesearch" value="http://yoursite.com"/></form></div><div class="widget"><div class="widget-title"><i class="fa fa-folder-o"> Kategorien</i></div><ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/Beijing/">Beijing</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/C语言学习笔记/">C语言学习笔记</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Security/">Security</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Spark/">Spark</a><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/categories/Spark/RDD/">RDD</a><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/categories/Spark/RDD/source-code/">source code</a></li></ul></li><li class="category-list-item"><a class="category-list-link" href="/categories/Spark/configuration/">configuration</a></li></ul></li><li class="category-list-item"><a class="category-list-link" href="/categories/blog/">blog</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/coding/">coding</a><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/categories/coding/compiler/">compiler</a></li></ul></li><li class="category-list-item"><a class="category-list-link" href="/categories/dp/">dp</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/language/">language</a><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/categories/language/Japanese/">Japanese</a><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/categories/language/Japanese/lyric/">lyric</a></li></ul></li></ul></li><li class="category-list-item"><a class="category-list-link" href="/categories/leetcode/">leetcode</a><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/categories/leetcode/HashTable/">HashTable</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/leetcode/Two-Pointers/">Two Pointers</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/leetcode/divide-and-conquer/">divide and conquer</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/leetcode/dp/">dp</a></li></ul></li><li class="category-list-item"><a class="category-list-link" href="/categories/lua/">lua</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/mapreduce/">mapreduce</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/music/">music</a><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/categories/music/guitar/">guitar</a></li></ul></li><li class="category-list-item"><a class="category-list-link" href="/categories/paper/">paper</a><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/categories/paper/NLP/">NLP</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/paper/scheduler/">scheduler</a></li></ul></li><li class="category-list-item"><a class="category-list-link" href="/categories/programming-language/">programming language</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/python/">python</a><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/categories/python/matplotlib/">matplotlib</a></li></ul></li><li class="category-list-item"><a class="category-list-link" href="/categories/software-engineer/">software engineer</a><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/categories/software-engineer/API/">API</a></li></ul></li><li class="category-list-item"><a class="category-list-link" href="/categories/source-code/">source code</a><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/categories/source-code/JXIO/">JXIO</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/source-code/Spark/">Spark</a></li></ul></li><li class="category-list-item"><a class="category-list-link" href="/categories/spark/">spark</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/unity/">unity</a><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/categories/unity/animation/">animation</a></li></ul></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-star-o"> Tags</i></div><div class="tagcloud"></div></div><div class="widget"><div class="widget-title"><i class="fa fa-file-o"> Letzte</i></div><ul class="post-list"><li class="post-list-item"><a class="post-list-link" href="/2017/10/28/unity-eventfunction/">unity_eventfunction</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/10/28/learn-lua/">lua学习</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/07/08/unity-animation/">unity-animation</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/04/09/dynamic-resource-allocation/">Dynamic Resource Allocation</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/03/06/swprintf/">来聊一聊C++的string</a></li><li class="post-list-item"><a class="post-list-link" href="/2016/08/13/beijing/">出去玩-北京</a></li><li class="post-list-item"><a class="post-list-link" href="/2016/05/31/running-spark-on-mesos/">Running Spark on Mesos</a></li><li class="post-list-item"><a class="post-list-link" href="/2016/04/12/spark-summary/">Spark小结</a></li><li class="post-list-item"><a class="post-list-link" href="/2016/03/25/the-number-of-the-increasing-sub-sequences/">一个数组的递增子序列的个数</a></li><li class="post-list-item"><a class="post-list-link" href="/2015/12/13/factorial-nonzero-suffix/">求N!的后9位不为0的数</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-external-link"> Blogroll</i></div><ul></ul><a href="http://www.example1.com/" title="site-name1" target="_blank">site-name1</a><ul></ul><a href="http://www.example2.com/" title="site-name2" target="_blank">site-name2</a><ul></ul><a href="http://www.example3.com/" title="site-name3" target="_blank">site-name3</a></div></div></div><div class="pure-u-1 pure-u-md-3-4"><div id="footer">Copyright © 2017 <a href="/." rel="nofollow">ShallWe Talk.</a> Powered by<a rel="nofollow" target="_blank" href="https://hexo.io"> Hexo.</a><a rel="nofollow" target="_blank" href="https://github.com/tufu9441/maupassant-hexo"> Theme</a> by<a rel="nofollow" target="_blank" href="https://github.com/pagecho"> Cho.</a></div><script src="//dn-lbstatics.qbox.me/lbservice/busuanzi/2.0/busuanzi.mini.js"/></script>
<span id="busuanzi_container_site_uv">已经有<span id="busuanzi_value_site_uv"></span>人次访问carolz的小站啦 ( •̀ ω •́ )y</span></div></div><a id="rocket" href="#top" class="show"></a><script type="text/javascript" src="/js/totop.js?v=0.0.0" async></script><script type="text/javascript" src="//cdn.bootcss.com/fancybox/3.1.20/jquery.fancybox.min.js" async></script><script type="text/javascript" src="/js/fancybox.js?v=0.0.0" async></script><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/fancybox/3.1.20/jquery.fancybox.min.css"><script type="text/javascript" src="/js/codeblock-resizer.js?v=0.0.0"></script><script type="text/javascript" src="/js/smartresize.js?v=0.0.0"></script></div></body></html>